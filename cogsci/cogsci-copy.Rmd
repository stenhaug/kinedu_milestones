---
title: "Torches and Pitchforks: A Bayesian Reevaluation of PISA"
bibliography: cogsci_ref.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Klint Kanopka (kkanopka@stanford.edu)} \\ Graduate School of Education, 485 Lausen Mall \\ Stanford, CA 94305 USA
    \AND {\large \bf Ben Stenhaug (stenhaug@stanford.edu)} \\ Graduate School of Education, 485 Lausen Mall \\ Stanford, CA 94305 USA}

abstract: >
    The Programme for International Student Assessment (PISA) is conducted every three years and used to provide
    educational comparisons between countries and rankings of their performance. Though the results are used to 
    inform educational policy across the world, the production of these scores is a pipeline of model fitting, 
    weighting, and multiple imputation that functions as a dark grey box. We investigate some of the counter intuitive
    results of the 2015 PISA administration and develop a fully Bayesian hierarchical model for comparing country level
    performance. Our model, while computationally more expensive, is significantly more transparent while also being
    more honest about uncertainty. We fit this model to a subset of the PISA data and results are examined for 
    agreement with the officially reported PISA results.
    
keywords: >
    PISA; psychometrics; Bayesian item response theory;
    
output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
theme_set(theme_classic(base_size = 8))

cog <- 
    read_csv(here::here("/data-clean/cog_students.csv")) %>% 
    filter(CNTRYID != "Spain (Regions)_duplicated_971")

country <- 
    read_csv(here::here("/data-clean/countries.csv")) %>% 
    filter(country != "Spain (Regions)_duplicated_971")

model <- read_rds(here::here("/data-model-output/2_12_10.rds"))

mapping <- read_csv(here::here("/data-modeling/mapping.csv"))

countries_to_run <- read_csv(here::here("/data-clean/countries_to_run.csv"))

long_model_subset <- read_csv(here::here("/data-modeling/long_model_subset.csv"))
```

# Introduction

The Programme for International Student Assessment (PISA) is conducted every three years and aims to measure "the extent to which 15-year-old students, near the end of the compulsory education, have acquired key knowledge and skills" [@pisa2015pisa]. PISA results are most commonly reported either in terms of the rank-ordering of countries or a score for each country placed on a scale with mean 500 and standard deviation 100. 

We initially concern ourselves with the 2015 PISA science content area, which was the main focus of the 2015 PISA administration. As an example, the Figure \ref{fig:image} gives the PISA-reported scores for the top 30 performing countries.  

```{r image, fig.cap = "2015 PISA Science Average Scores. Top 30 countries."}
country %>% 
    arrange(desc(pisa)) %>% 
    slice(1:30) %>% 
    mutate(country = fct_reorder(country, pisa)) %>% 
    ggplot(aes(x = country, y = pisa)) +
    geom_point() +
    coord_flip() +
    labs(
        x = "",
        y = "",
        title = ""
    ) +
  geom_hline(yintercept = 500, color = "gray", linetype = "dashed")
```

# Sampling

In this section, we provide an overview of the sampling and weighting strategy from the PISA 2015 technical report [@pisa2015tech]. Our goal is to provide just enough detail in order to understand how this process impacts the analysis and results.

Stratification in sampling is used to ensure that the sample of students is representative of the country's population of school-going 15-year-olds and to increase the reliability of the survey estimates. A minimum of 150 schools were required per country. Generally 42 students were selected from each school. This leads to a minimum sample size of 5,250 students.

With the exception of Russia, all countries used a two-stage sampling design where schools were sampled in the first stage and students were sampled in the second stage. First, each school in a country is put into a mutually exclusive group (PISA refers to these groups as "explicit strata") most typically defined by state or region. The number of schools sampled from each explicit strata is based on the proportion of PISA-eligible students attending school in that explicit strata. For example, Argentina creates explicit strata based on six regions. The number of explicit strata that a country uses varies widely, from as low as two to as many as 96. Second, the schools in each explicit strata are spaced by secondary factors (PISA refers to these as "implicit strata variables") such as type of school. For example, Argentina uses school funding, the level of education a school offers, if a school is urban, and whether a school is secular or religious as implicit strata variables. Schools in each explicit strata are then sampled according to an even-spacing such that schools are selected with a good mix of implicit strata variables. Due to its size, Russia used a three-stage sampling design where they first sampled regions, then schools, and finally students.

There are a variety of issues that can complicate this otherwise straightforward sampling design. For example, PISA requires that 85% of selected schools and 80% of selected students from each selected school actually take the PISA. In some cases, a low-response rate can be improved by selecting replacement schools. As another example of complexity, countries generally prefer not to select too many small schools because it can be administrative burden. Therefore, in countries where many PISA-eligible students attend small schools, either small schools were undersampled and the number of sampled schools was increased (somewhat rare) or small schools were not undersampled but the school sample size was increased above 150 (somewhat more common).

# Weighting

Weights must be applied to each student so that the sample appropriately represents the country's population of PISA-eligible students. PISA gives three reasons for this. First, some schools may have been intentionally over or under-sampled. For example, a country might over-sample some schools for national purposes or, as described in the previous section, small schools might be undersampled to avoid administrative burden. Second, school and student non-response needs to be mitigated by up-weighting similar schools/students. Last, the information about school size at the time of sampling may have been imperfect leading to incorrect probabilities being assigned during the sampling process. To prevent a student or school from having too-heavy an influence on a country's results, the weights were trimmed so that an individual student has at-most four times the median weight of all students in their explicit strata. PISA reports that trimming was only necessary for a few students in each of about 15\% of countries. Note that because trimming only happened at the explicit strata level it is still possible to have large differences in weights for students from the same country if they come from different explicit strata. A few countries in particular have a large amount of variation in weights, with the most extreme being Beiging-Shanghai-Jiangsu-Guangdon China. This is depicted in Figure \ref{fig:allweights}.

```{r allweights, fig.cap = "Distribution of weights by country"}
cog %>% 
  filter(CNTRYID %in% (country %>% arrange(desc(pisa)) %>% slice(1:30) %>% pull(country))) %>% 
    group_by(CNTRYID) %>% 
    mutate(weight = weight / min(weight)) %>% 
    ungroup() %>% 
    mutate(CNTRYID = fct_reorder(CNTRYID, weight, max)) %>% 
    ggplot(aes(x = CNTRYID, y = weight)) +
    geom_boxplot() +
    coord_flip() +
    labs(
        y = "Weight",
        x = "",
        title = ""
    )
```

We can get a better feel for the distribution of weights by looking at the full histogram for a few sample countries as is shown in Figure \ref{fig:fourweights}.

```{r fourweights, fig.cap = "Distribution of weights for a few countries"}
cog %>% 
    group_by(CNTRYID) %>% 
    mutate(weight = weight / min(weight)) %>% 
    ungroup() %>% 
    filter(CNTRYID %in% c("Japan", "Austria", "Portugal", "United States")) %>% 
    mutate(CNTRYID = factor(CNTRYID, levels = c("Japan", "Austria", "Portugal", "United States"))) %>% 
    ggplot(aes(x = weight)) +
    geom_histogram() +
    facet_wrap(~ CNTRYID, ncol = 2) +
    scale_y_log10() +
    labs(
        x = "Weight",
        y = "Number of students (log scale)"
    )
```

# Sum-score based results

Each selected student is randomly assigned one of 36 test versions which determines the exact set of 25-36 science items that they receive. Though students are given one hour to complete their science items, the majority complete the assessment well before that, with a median time of approximately 45 minutes. Additionally, 13 of the 182 items offer partial credit which is earned by fewer than $25\%$ of students attempting the item. For simplicity, we collapse partial credit into correct so that all items have a simple, dichotomously coded, correct or incorrect grading scheme.

First we look at performances by Austria and Portugal. For example, test version 27 contains 33 questions. In Austria, the 169 students taking test version 27 score a mean of 15.08 questions correct. In Portugal, the 184 students taking test version 27 score a mean of 11.25. To maintain comparability with other test versions with different number of questions, we calculate the scaled score for Austria as $\dfrac{15.08}{33} \times 100 = 45.7$ and for Portugal as $\dfrac{11.25}{33} \times 100 = 34.1$.

The left of Figure \ref{fig:austria} shows the differences in these scale scores by test version. Bars that extend further to the right indicate Austrian students, on average, answered more items correctly than Portuguese students. As an example, consider the top bar corresponding to test version 27 reports a difference between Austria and Portugal of $45.7 - 34.1 = 11.6$. The values are unweighted because we have not taken into account the student weights as described in the previous section. The right shows the difference in scaled scores after applying the student weights. Both distributions find that Austria outperforms Portugal on most test versions with the difference being a bit less pronounced after taking into account the student weights.

```{r austria, fig.cap = "Austria outperforms Portugal on most test versions"}
subset_countries <- 
    cog %>% 
    filter(CNTRYID %in% c("Austria", "Portugal")) %>% 
    group_by(CNTRYID, cluster, out_of) %>% 
    summarize(
        unweighted = mean(correct / out_of) * 100,
        weighted = weighted.mean(correct / out_of, weight) * 100,
        n = n()
    ) %>% 
    ungroup()

bind_rows(
    subset_countries %>% 
        filter(cluster != 0) %>% 
        group_by(cluster) %>% 
        summarize(diff = weighted[1] - weighted[2]) %>% 
        mutate(which = "Weighted"),
    subset_countries %>% 
        filter(cluster != 0) %>% 
        group_by(cluster) %>% 
        summarize(diff = unweighted[1] - unweighted[2]) %>% 
        mutate(which = "Unweighted")
) %>% 
    mutate(
        cluster = cluster %>% factor() %>% fct_reorder(diff, function(x) x[2])
    ) %>% 
    ggplot(aes(x = cluster, y = diff)) +
    geom_col() +
    coord_flip() +
    facet_wrap(~ which) +
    labs(
        x = "Test version",
        y = "Difference in scaled test score (Austria - Portugal)"
    )
```

We can calculate the weighted average scale score for each country and then compare it to the PISA science score. In general, there is a strong but imperfect relationship which is shown in Figure \ref{fig:relat}.

```{r relat, fig.cap = "Relationship between PISA science score and average scaled score"}
country %>% 
    ggplot(aes(x = pisa, y = scaled_weighted)) +
    geom_point() +
    labs(
        x = "PISA science score",
        y = "Average scaled score (weighted)"
    )
```

In particular, zooming into a smaller range of countries shows that there are many cases of disagreement between averaged weighted scaled score and the PISA science score. Figure \ref{fig:zoom} shows this relationship for countries that had a PISA science score between $490$ and $501$. Austria and Portugal, as described, is one such example of a reversal.

```{r zoom, fig.cap = "Results from countries of interest"}
country %>% 
    filter(pisa >= 490, pisa <= 501) %>% 
    ggplot(aes(x = pisa, y = scaled_weighted)) +
    geom_point() +
    ggrepel::geom_text_repel(aes(label = country), size = 2) +
    labs(
        x = "PISA science score",
        y = "Average scaled score (weighted)"
    )
```

Of course, the weighted average scaled score is a statistic simply based on the number of correct responses, which PISA warns against because students take different test versions which have varying difficulties. We note here that while that is true, students from each country are *randomly assigned* to these test forms. To mitigate the issue of varying difficulty in test versions, PISA uses item response theory (IRT) which creates a common scale both for test versions and students. 

# PISA Scaling

To arrive at the reported scores, PISA engages in a four stage scaling process. First, they fit an IRT model to the items to allow for the creation of a common scale across forms. Second, they perform a differential item functioning (DIF) analysis to ensure comparability of item responses across countries. Third, they perform a latent variable regression to develop posterior distributions of ability for students. Finally, they draw plausible values from these posteriors to estimate mean abilities at the country level for the purposes of reporting and comparison.

## Fitting IRT Models

PISA actually employs four separate item response functions (IRFs) for scaling purposes. Trend items, which are used to equate scores across administrations, are legacy-scored using a Rasch model for dichotomous items and a partial credit model (PCM) for polytomously scored items. New items for the 2015 administration are scored using a two parameter logistic IRF (2PL) for dichotomous items or the generalized partial credit model (GPCM) for polytomously scored items. Additionally, for trend items, if the Rasch/PCM IRF shows poor fit, they are rescaled using the 2PL/GPCM. Once an IRF has been chosen for each item, a model is fit to estimate world-level item parameters. After this model is fit, the same IRFs are used to estimate item parameters within each country $\times$ language group. This is especially important in countries like Belgium, where PISA is administered in multiple languages and exchangeability across test translations cannot be assumed. Additionally, during the fitting of the IRT models, student weights within each country $\times$ language are standardized and used.

## DIF Analysis

To check for DIF, PISA uses an IRT approach. For each item, the item characteristic curve (ICC) determined by the world-level parameters is compared to the ICC from each set of country $\times$ language parameters. An IRF-specific measure of ICC similarity is computed between each country $\times$ language ICC and the world ICC, and subgroups that show substantial misfit are isolated for each item. When this misfit is detected for a specific country $\times$ language group, the item is split off from the world parameters for this group. This results in the country $\times$ language-specific item parameters being retained and the item treated as a unique item that is only seen by that specific country $\times$ language group. While this process reduces the number of items that can be used to compare groups, it increases the precision with which individual abilities can be estimated. Additionally, this only occurs for approximately 10\% of items.

## Latent Variable Regression

Within each country $\times$ language group, the items parameters estimated from the previous steps are held as fixed and a latent variable regression is performed to estimate the relationship between ability, $\theta$, and demographics, $y$. Specifically, PISA assumes:

$$
\theta \sim \mathcal{N}(\Gamma y, \Sigma)
$$

Where $\Gamma$ are weights and $\Sigma$ is a covariance matrix, both of which are estimated from the latent variable regression. 

The student demographic vectors, $y$, are not raw demographics, but instead a derived from a principal component analysis (PCA) on student responses to the PISA survey and measurements of student response behavior -- e.g., number of omitted items. From the PCA, the first principal components that explain 80\% of the variance are retained. Because there are two different versions of the PISA survey and response heterogeneity varies by country $\times$ language group, different country $\times$ language groups will retain different numbers of principal components. Therefore the $y$ vectors (and subsequently weights, $\Gamma$, and covariance, $\Sigma$) are not comparable across country $\times$ language groups. This functions as a matching tool to help compare students who took different sets of items within the same country $\times$ language group.

## Plausible Values

Once item responses, item parameters, and the latent variable regression weights are determined, a posterior over ability for each student is constructed. Specifically, for a student $j$, PISA constructs the posterior:

$$
P(\theta_j | x_j, y_j, \Gamma, \Sigma)
$$

where $x_j$ is student $j$'s item responses, $y_j$ are the principal components of their demographics, $\Gamma$ is the vector of weights from their country $\times$ language group's latent variable regression, and $\Sigma$ is their country $\times$ language group's covariance matrix. From these posteriors, ten draws (called "plausible values") are taken and used to approximate the ability distribution for the country. From these plausible values, country means and standard errors are determined, which are then rescaled into reported scores and rankings.

# Bayesian Modeling

A more straightforward and transparent yet computationally expensive way of analyzing the PISA item response data is to invoke a fully Bayesian model. In this section, we report our results from fitting a fully Bayesian model to a subset of the data. In particular, we subset to the 10 countries shown in Figure \ref{fig:zoom} and randomly sample 1000 students from each of those countries. Model fitting is done in stan. 

We define a hierarchical Bayesian model where student $i$ from country $c$ has ability $\theta_{ci}$ which is drawn from their country distribution $\theta_{ci} \sim N(\mu_c, \sigma_c)$. We model response probability using a 3PL IRT model where an item $j$'s difficulty $b_j$, discrimination $a_j$, and guessability $g_j$ are the same for all students regardless of which country they are from. The full model is specified as follows:


$$
\mu_c \sim \mathcal{N}(0, 4)
$$
$$
\sigma_c \sim \mathcal{N}(1, 4)
$$
$$
\theta_{ci} \sim \mathcal{N}(\mu_c, \sigma_c)
$$
$$
b_i \sim \mathcal{N}(0, 2)
$$
$$
a_i \sim N(1, 1)
$$
$$
g_i \sim \text{beta}(5, 17)
$$
$$
y_{cij} \sim \text{Bernoulli}(\eta_{cij})
$$
$$
\text{where}
$$
$$
\eta_{cij} = g_i + (1 - g_i) \cdot \sigma(a_j \theta_{ci} + b_i)
$$
$$
\sigma(z) = \frac{1}{1+\exp(-z)}
$$

\ref{fig:outcomes} shows the relationship between the PISA science score and $E[\mu_{c}]$ from the fully Bayesian model.

```{r outcomes, fig.cap = "Bayesian results"}
country_draws_mu <- 
    model %>% 
    recover_types() %>% 
    spread_draws(mu_country[group]) %>% 
    filter(.chain != 3) %>% # REMEMBER TO DELETE THIS IN THE FUTURE
    left_join(mapping, by = "group") %>% 
    ungroup()

results <- 
    countries_to_run %>% 
    left_join(
        country_draws_mu %>% 
            group_by(country) %>% 
            summarize(mcmc_mu = mean(mu_country))
    ) %>% 
    left_join(
        long_model_subset %>% 
            left_join(mapping) %>% 
            group_by(country) %>% 
            summarize(correct_rate_sample = mean(correct))
    )

results %>% 
    ggplot(aes(x = pisa, y  = mcmc_mu)) +
    ggrepel::geom_text_repel(aes(label = country), size = 2) +
    geom_point() +
    labs(
        x = "PISA science score",
        y = latex2exp::TeX("E\\[mu_{c}\\]")
    )

results %>% 
  ggplot(aes(x = scaled, y  = mcmc_mu)) +
  geom_point()
```

Note that in this plot, $\mu_{Austria}$ is fixed to be zero.

# Moving Forward

We have identified four major points of divergence between our model and the PISA scaling procedure that could account for the differences we observed. First, we did not use the full dataset and only fit our model to a subset of countries and students. Second, we fit a 3PL IRF for all items instead of considering partial credit and fitting Rasch/PCM or 2PL/GPCM models to trend and new items, respectively. Third, we did not account for DIF among country $\times$ language groups. Finally, we did not apply a weighting to observations that reflects the PISA sampling scheme. Our next steps are designed around either incorporating new pieces that address these differences or constructing experiments to explore how reasonable our modeling decisions were.

## Full Data

The results reported here were fit using stan in R on local machines. In order to scale up to the full dataset, we now have a functional Hamiltonian Monte Carlo (HMC) implementation in Pyro. This model runs on GPUs on Sherlock, with some caveats. First, there is a bug that prevents it from running more than one chain at a time, making it strictly less useful than the current stan implementation. In order to move to the full data set, this bug must be squashed and the implementation needs to be imported to numpyro, because the HMC implementation is significantly faster in numpyro than in standard Pyro.

## Item Response Functions

Our second major difference is the use of the 3PL IRF. Its blanket use has one major downside from PISA's perspective, in that it removes their ability to equate across years. From our perspective, however, PISA is a low-stakes test that does not report individual student performance, so significant amounts of guessing ought to be expected. From our own results, estimated guessing parameters were modest, but nonzero. Moving forward, we will fit 2PL/GPCM models for comparison with our own 3PL-based model.

## DIF

The DIF identification procedure is central to PISA's scaling, but we have completely ignored it for initial simplicity. To include the ability to allow country $\times$ language groups, we can specify another hierarchical model. To illustrate, consider item $i$'s difficulty parameter, denoted as $b_i$. As before we specify a prior on $b_i$:

$$b_i \sim \mathcal{N}(0, 2)$$
Next we define a country $\times$ language group's specific item parameter as $b_{ic}$, and specify its distribution as:

$$b_{ic} \sim \text{Laplace}(b_i, 1)$$
Putting a Laplace prior on the country $\times$ language group item parameters functions like LASSO (or $L_1$) regularization - given enough data, the parameter is allowed to vary from the world value, but it prefers to be exactly the same. This hierarchical specification will also work for the discrimination parameters, $a_{ic}$, and guessing parameters, $g_{ic}$ and is consistent with PISA's approach to handling observed DIF.

## Weighting

Including weighting in the fully Bayesian scheme is less straightforward than some of the other modifications. As it stands, our best idea is to apply the weights after fitting our model to generate comparable weighted posteriors.

# Discussion

After examining a subset of the 2015 PISA science data, we observe some counterintuitive results being reported. Outlining the PISA scaling procedure, we find a number of places where interpretability and transparency can be improved. As there is no free lunch in psychometrics, this defensible transparency comes at a computational cost, paid in the form of a fully Bayesian hierarchical item response theory model. Beyond interpretability, we believe our model also provides an advantage in that it models guessing, which should be expected on a widely administered low stakes exam. In comparing our results to the reported PISA results, we find the Bayesian model produces rankings more in line with intuitions derived from investigating sum scores. Our model is limited in that it does not take into account PISA sampling procedures, DIF, or use the full dataset. As such, extensions are planned to make our model even more comparable with existing PISA machinery.

# Acknowledgements

We'd like to thank Mike Frank, Noah Goodman, and the Psych 241 class for their contributions to this work.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
