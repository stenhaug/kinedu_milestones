---
title: "The latent factor structure of child development"
bibliography: cogsci_ref.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{Anonymous Cogsci Submission}

abstract: >
    \myworries{to do}
    
keywords: >
    child development; milestones; item response theory; model comparison
    
output: cogsci2016::cogsci_paper
header-includes:
- \usepackage{amsmath}
- \usepackage{bm}
- \usepackage{xcolor}
- \newcommand\myworries[1]{\textcolor{red}{#1}}
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)

library(tidyverse)
theme_set(theme_classic())

# load in
areas <- read_rds(here::here("data-clean/areas.rds"))
d <- read_rds(here::here("data-clean/d.rds"))
d_mat <- read_rds(here::here("data-clean/d_mat.rds"))
item_area <- areas$area[match(colnames(d_mat), areas$paste)]

fix_names <- function(x) ifelse(x == "", paste0("X__", 1:length(x) - 1), x)

d_raw <- 
  readxl::read_xlsx(
    here::here("data/norming2/Kinedu Norming Survey Raw Data - May 15 2018.xlsx"),
    .name_repair = fix_names
  )

# remove ages < 1
ages <-
    d %>%
    group_by(id) %>%
    summarize(age = age[1]) %>%
    filter(age > 1)

d_mat <- d_mat[row.names(d_mat) %in% ages$id, ]

d <- d %>% filter(age > 1)

# get milestones
milestones <-
    d_raw %>%
    select(abs_183:color_679) %>%
    slice(3) %>%
    gather(code, name) %>%
    mutate(short_name = str_sub(name, start = 0, end = 40),
           code = str_replace(code, "^d_","d"),
           code = str_replace(code, "^e_","")) %>%
    mutate(code2 = code) %>%
    separate(code2, into = c("category","number")) %>%
    select(-number)

ms <- milestones %>% mutate(code = str_remove(code, " "))
```

# Introduction

\myworries{to do}

# Data

A child’s development can be thought of as the set of developmental milestones that they have reached at a particular point in time. This conceptualization results in data with the same structure as the item response data common to educational measurement. In education, item response data is most typically students responding to test items (i.e., questions) and, in the dichotomous case, getting each question either correct or incorrect. In the context of child development, the child is the “student,” and each developmental milestone is the “item.”

We use Kinedu, a Mexico-based child development app, as a source for this type of data. When parents first start using the Kinedu app, they are asked a series of questions about which developmental milestones their child has reached. We consider the 1946 children between 2 and 55 months of age whose parents responded to all 414 of the developmental milestones. Each developmental miletone on Kinedu is mapped to a milestone group: physical, cognitive, linguistic, or social & emotional. Table \ref{tab:examples} shows the number of developmental milestones in each group along with an example milestone translated to English.

```{r examples}
areas %>%
    filter(paste %in% colnames(d_mat)) %>%
    group_by(area) %>%
    mutate(count = n()) %>%
    slice(1) %>%
    ungroup() %>%
    left_join(ms, by = c("paste" = "code")) %>%
    select(Group = area, Count = count, spanish = short_name) %>%
    select(-spanish) %>% 
    arrange(desc(Count)) %>%
    mutate(`Example milestone` = c(
        "Stands on their toes",
        "Finds objects on the floor",
        "Babbles to imitate conversations",
        "Complains when play is interrupted"
    )) %>% 
    knitr::kable("latex", caption = "Developmental milestone groups and examples") %>% 
    kableExtra::kable_styling(font_size = 8, latex_options = "hold_position")
```

Figure \ref{fig:growth} shows the age (in months) and number of developmental milestones for each child. At 12 months old, most children have reached about 200 developmental milestones. At 24 months old, most children have reached about 300 developmental milestones. Finally, at 48 months old, most children have reached about 375 of the 414 developmental milestones.

\myworries{probably should describe where}
\myworries{the percentile curves come from}

```{r growth, fig.cap = 'Number of milestones by age', warning = FALSE}
source(here::here("data-emailed-from-mike/predictQR_fixed.R"))
library(quantregGrowth)

by_age <- 
    d %>%
    group_by(id) %>%
    summarise(
        age = age[1], 
        response = sum(response)
    )

excl <- 
    by_age %>%
    filter(age <= 1) %>%
    pull(id)

by_age <- filter(by_age, !(id %in% excl))
d <- filter(d, !(id %in% excl))

taus <-  c(0.1, 0.25, 0.5, 0.75, 0.9)

mod <- gcrq(formula = response ~ ps(age, monotone = 1, lambda = 1000), 
     tau = taus, data = by_age)

the_ages <- 1:55
newdata <- data.frame(age = the_ages)

preds <- predictQR_fixed(mod, newdata = newdata) %>%
  data.frame %>%
  mutate(age = the_ages) %>%
  gather(Percentile, pred, starts_with("X")) %>%
  mutate(Percentile = as.character(as.numeric(str_replace(Percentile, "X", "")) * 100))

ggplot(by_age, 
       aes(x = age, y = response)) + 
  geom_jitter(height = 0, width = 0, alpha = .1) +  # was width = 0.2 and alpha 0.3
  geom_line(data = preds, aes(x = age, y = pred, col = Percentile, group = Percentile)) + 
  ylim(0,nrow(milestones)) + 
  xlim(0,55) + 
  ylab("Total milestones") + 
  xlab("Age (in months)") + 
  ggthemes::scale_color_solarized() + 
  theme(legend.position = "bottom", 
        legend.title = element_text(size = 6),
        legend.text = element_text(size=6))
```

# Empirical assessment of the dimensionality of child development

We frame the assessment of the dimensionality of child development as a model comparison question. 

## Models

Item response theory offers a suite of models with which to model item response data. We adopt the notation used in @chalmers2012mirt. Let $i = 1, \ldots, I$ represent the distinct children and $j = 1, \ldots, J$ the developmental milestones. The Kinedu item response data is stored in a matrix, $y$, where element $y_{ij}$ denotes if the $i$th child has or has not achieved the $j$th developmental milestone as reported by their parent/guardian. Each model represents the $i$th child's development using $m$ latent factors $\boldsymbol{\theta}_{i}=(\theta_1, \dots, \theta_m)$. The $j$th milestone's discriminations (i.e. slopes) $\boldsymbol{a_j}=(a_1, \dots, a_m)$ capture the latent factor loadings onto that milestone.

We fit four two-parametric logistic (2PL) models where a child’s development is represented by $m = 1, \ m = 2, \ m = 3,$ and $m = 4$ latent factors. Hereafter, we, for example, refer to a 2PL model with $m = 4$ latent factors as a 4F 2PL model. According to the 2PL model, the probability of a child having achieved a developmental milestone is
$$
P(y_{ij} = 1 | \boldsymbol{\theta_i}, \boldsymbol{a_j}, b_j) = \sigma(\boldsymbol{a}_{j}^{\top}\boldsymbol{\theta_i} + b_j)
$$
where $b_j$ is the milestone easiness (i.e. intercept) and $\sigma(x) = \frac{e^x}{e^x + 1}$ is the standard logistic function. We also fit a 1F Rasch model where each of the discriminations is fixed to 1.

The 2PL models learn the latent factor structure entirely from the data, making them exploratory. The bifactor model offers an alternative specification where each milestone loads onto a general factor $\theta_0$ and a specific factor $\theta_s$ [@cai2011generalized]. The assignment of each developmental milestone to its specific factor is an opportunity to specify the latent factor structure, making the model confirmatory as opposed to exploratory. We map each milestone to its specific factor according to the four developmental milestone groups shown in Table \ref{tab:examples}. For the bifactor model, the probability of a child having achieved a developmental milestone is 
$$
P(y_{ij} = 1 | \theta_0, \theta_s, a_0, a_s) = \sigma(a_0\theta_0 + a_s\theta_s + b_j).
$$

## Model comparison {#modelcompare}

Model comparison in IRT typically uses information criterion such as AIC and BIC [@maydeu2013goodness]. However, these methods are not guaranteed to work with modest sample sizes or misspecification [@mcdonald1995goodness]. Instead, we prefer a marginalized version of cross-validation. In essence, we partition the data into folds based on the children (i.e. the rows of the item response matrix). Then for each fold, we estimate the item parameters using all but that fold, and calculate the likelihood of that fold by integrating over $g(\theta)$. 

Mathematically and following notation similar to @vehtari2017practical, we partition the data into $K$ subsets $y^{(k)}$ for $k = 1, \dots, K$. Each model is fit separately to each training set $y^{(-k)}$ yielding item parameter estimates which we compactly denote $\Psi_j^{(-k)}$. The predictive (i.e. out-of-sample or cross-validated) likelihood of $y^{(k)}$ is

$$
p(y^{(k)} | y^{(-k)}) = \prod_{i \in i^{(k)}}^{I} \int_\theta \prod_{j=1}^{J} \hat{\text{Pr}}(y_{ij}^{(k)} | \Psi_j^{(-k)}, \theta) g(\theta)d\theta.
$$

The ultimate quantity of interest for each model is the log predictive likelihood for the entire item response matrix, which is defined as

$$
\text{lpl } y = \sum_{k = 1}^{K} \log p(y^{(k)} | y^{(-k)}).
$$

## Results

\myworries{fascinating that dropping 1 month olds}
\myworries{changed the winner from 3F to 4F}

Table \ref{tab:results} shows the number of parameters, the in-sample log likelihood (which neccessarily increases with more parameters), and the $\text{lpl } y$ defined in the [model comparison section](#modelcompare). The 4F 2PL model performs best which is evidence that child development between the ages of 2 and 55 months follows a multidimensional path. 

\myworries{need to tie results to literature}

Computing is done in R [@rcore], model fitting in the R package mirt [@chalmers2012mirt], and data wrangling/visualization in the set of R packages known as the tidyverse [@tidy].

\myworries{think about presentation of this table.}
\myworries{is it worth displaying in sample numbers?}
\myworries{would it be better as a graph?}

```{r results}
# will need latek in kables:
# https://stackoverflow.com/questions/49416492/latex-formulas-or-symbols-in-table-cells-using-knitr-and-kableextra-in-r-markdow

mirts <- read_rds(here::here("02_mirts.rds"))
bifactor <- read_rds(here::here("02_bifactors.rds"))

full <-
    bind_rows(mirts, bifactor) %>%
    rename(out_of_sample = ll_person_item) %>%
    mutate(
        in_sample = exp(log_lik / nrow(d_mat))^(1/ncol(d_mat))
    ) %>%
    mutate(
        oos =
            splits_with_log_lik %>% map_dbl(~ sum(.$log_lik_test))
    ) %>%
    select(
        factors, itemtype, in_log_lik = log_lik, in_p = in_sample,
        out_log_lik = oos, out_p = out_of_sample, model_full, fscores, splits_with_log_lik)

full %>%
    mutate(npars = model_full %>% map_int(~ .@Model$nestpars)) %>%
    arrange(npars) %>%
    mutate(model = c("1F Rasch", "1F 2PL", "2F 2PL", "Bifactor", "3F 2PL", "*4F 2PL*")) %>%
    select(model, npars, in_log_lik, `lpl y (out-of-sample)` = out_log_lik) %>% 
  knitr::kable("latex", caption = "Model performance: The 4F 2PL performs best") %>% 
    kableExtra::kable_styling(font_size = 10, latex_options = "hold_position")
```

## Understanding the latent factor structure

\myworries{section is written as if 3F is winner}
\myworries{may need to edit to 4F}

To understand each of the three factors in the best performing model, we fit the model to the the full dataset. We then estimate the factor loadings (i.e. discriminations or slopes) using a varimax rotation. The varimax rotation results in orthogonal and, therefore, more interpretable factors [@kaiser1959computer]. Figure \ref{fig:factorloadings} shows the distribution of factor loadings for each group on each of the three factors. The first factor load mainly on cognitive and linguistic milestones. The second factor is a combination of each of the groups with the strongest loadings on the physical and social & emotional milestones. The third mainly load positively on linguistic milestones and, interestingly, negatively on physical milestones. 

```{r, include = FALSE}
library(mirt)
# TO DO: get mod3 directly from results - didn't need to fit again
mod3 <- read_rds(here::here("scratch_mod3.rds"))
mod3_summary <- summary(mod3, rotate = "varimax")
```

```{r factorloadings, fig.cap = 'Factor loadings by group'}
mod3_summary_with_areas <-
    mod3_summary$rotF %>%
    as_tibble() %>%
    mutate(paste = row.names(mod3_summary$rotF)) %>%
    left_join(areas)

mod3_summary_with_areas %>%
    select(Group = area, F1, F2, F3) %>%
    rename(`Factor 1` = F1, `Factor 2` = F2, `Factor 3` = F3) %>% 
    gather(var, val, -Group) %>%
    mutate(val = -val) %>%
    ggplot(aes(x = val, fill = Group)) +
    ggridges::geom_density_line(alpha = 0.5) +
    facet_wrap(~ var, ncol = 1) +
    labs(
        x = "Factor loading (i.e. discrimination or slope)",
        y = "Density",
        title = "Milestone loadings by factor"
    ) +
    theme_classic(base_size = 8) + 
    theme(legend.position = "bottom", 
          legend.title = element_text(size = 6),
          legend.text = element_text(size=6),
          legend.key.size = unit(0.2, "cm"))
```

We also estimate the factor scores for each child using expected a posteriori (EAP) with a three dimensional standard normal distribution [@embretson2013item].  Figure \ref{fig:factorscores} shows the relationship between age and factor score for each factor. The first factor, perhaps unsurprisingly, has a high correlation (r = 0.90) with age. The second factor has a strong association with age from 2 to 16 months but thereafter is unrelated to age. By and large, the third factor does not have any association with age.

```{r factorscores, fig.cap = 'The first factor is highly associated with age'}
f <- fscores(mod3, rotate = "varimax")

# to find that correlation of 0.9 above
# tmp <- f %>% as_tibble() %>% mutate(age = ages$age)
# cor(hi$F1, hi$age)

f %>%
    as_tibble() %>%
    mutate(age = ages$age) %>%
    rename(`Factor 1` = F1, `Factor 2` = F2, `Factor 3` = F3) %>% 
    gather(var, val, -age) %>%
    mutate(val = -val) %>%
    ggplot(aes(x = age, y = val)) +
    geom_point(alpha = 0.1) +
    facet_wrap(~ var, ncol = 1) +
    geom_smooth() +
    labs(
        x = "Age (in months)",
        y = "Factor score"
    )
```

# Dimensionality across the age-span

For all of the data, we’ve shown evidence that 4-factors performs best. But is this latent factor dimensionality consistent across age-span? For example, perhaps for very young children 1-factor is sufficient and then later on 2- and then 3-factors become valuable. We take two approaches to assessing the dimensionality of child development across the age-span. First, we examine the performance of each of the models by age. Second, we partition the data by age and use the same cross-validation procedure to find the best fitting model in each partition.

## Full model {#full}

\myworries{figure currently shows up at end of paper.}
\myworries{if we go with this graph,}
\myworries{probably want more smoothing}

Figure \ref{fig:byage} displays the mean cross-validated log likelihood for each model by age, which comes from the k-fold cross-validation described in the [model comparison](#modelcompare) section. For each student, we calculate the marginalized out-of-sample likelihood based on the item parameters $\Psi_j^{(-k)}$ from fitting the model to $y^{(-k)}$, the folds of data that does not include the student. As a reminder, students are assigned to folds randomly and not by age. 

Figure \ref{fig:byage} shows that cross-validated log likelihood is highest for very young (less than 8 months old) and older (greater than 48 months old) children. This is not surprising as the number of milestones for each of these age groups is relatively close to 0% and 100% respectively. More importantly, we see that the 2F 2PL model performs best for children two to six months old. The 2F 2PL model performs as well as the 4F 2PL model for children from six months old to 11 months old. For children older than 11 months old, the 4F 2PL model tends to perform best. Interestingly, the bifactor model performs more similarly across the age-span to the 4F 2PL than the 2F 2PL despite having a similar number of parameters to the 2F 2PL. 

```{r byage, fig.cap = 'More flexible models perform better at older ages'}
read_rds(here::here("03_model_by_age.rds")) %>%
    filter(model %in% c("2F 2PL", "4F 2PL", "Bifactor")) %>%
    mutate(model = factor(model, levels = c("2F 2PL", "Bifactor", "4F 2PL"))) %>% 
    select(model, oos) %>%
    unnest(oos) %>%
    mutate(oos = log(oos)) %>%
    group_by(model, age) %>%
    summarize(m = mean(oos)) %>%
    ungroup() %>%
    rename(Model = model) %>% 
    ggplot(aes(x = age, y = m, color = Model)) +
    geom_point(size = 0.75) +
    geom_path(size = 0.35) +
    labs(x = "Age (in months)", y = "Mean cross-validated log likelihood") +
    theme_classic(base_size = 10) + 
    theme(legend.position = "bottom", 
            legend.title = element_text(size = 8),
            legend.text = element_text(size = 8),
            legend.key.size = unit(0.3, "cm"))
```

## Age-partitioned models

\myworries{figure currently shows up at end of paper.}
\myworries{need to add in 4F models}

As another method of examining the dimensionality of child development across the age span, we create four partitions of the data based on the ages of the children. We then cross-validate the 2PL models independently in each partition. This allows us to examine the dimensionality for each age group separately. For each age partition, we drop milestones where less than 2.5% or greater than 97.5% of children have reached the milestone. Dropping milestones is done because these milestones do not contain much information and make IRT models less stable. This process results in, for example, 432 children and 359 milestones in the 13-24 month old partition.

\myworries{should we note (or sensitivity check)}
\myworries{the tendency smaller datasets}
\myworries{to prefer less flexible models?}

Figure \ref{fig:partage} shows the results of this analysis. Consistent with our findings in the [previous section](#full), the best fitting model contains a lower dimensional factor structure for younger children. The best fitting model is the 2F 2PL for the partition of data containing children two to 12 months old, whereas the best fitting model is the 3F 2PL for the partitions containing older children.

```{r partage, fig.cap = '2F 2PL best for young kids; 3F 2PL best for older kids'}
splits_by_age <- read_rds(here::here("archived/13_splits_by_age.rds"))

add_lltest <- function(a){
  a %>% 
  mutate(
    log_lik_test = 
      splits_with_log_lik %>% 
      map_dbl(~ sum(.$log_lik_test))
  )
}

b <- splits_by_age %>% 
  mutate(models2 = models %>% map(add_lltest))

tibble(
  years = c("2-12 months", "13-24 months", "25-36 months", "37+ months")
) %>% 
  mutate(
    kids = splits_by_age$d_mat_filter %>% map_int(nrow),
    items = splits_by_age$d_mat_filter %>% map_int(ncol),
    m  = b$models2
  ) %>% 
  unnest() %>% 
  select(years, kids, items, factors, itemtype, log_lik, log_lik_test) %>% 
  mutate(var = as_factor(paste0(years, " ", kids, " kids", " ", items, " items"))) %>% 
  filter(itemtype == "2PL") %>% 
  ggplot(aes(x = factors, y = log_lik_test)) +
  geom_point() +
  geom_line(aes(group = years)) +
  facet_wrap(~ var, ncol = 1, scales = "free") +
  labs(
    x = "m-factor 2PL model",
    y = "Cross-validated log likelihood"
  ) +
  theme_classic(base_size = 8)
```

# Discussion

\myworries{to do}

# Acknowledgements

We'd like to thank Kinedu for providing the data that made this research possible.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
