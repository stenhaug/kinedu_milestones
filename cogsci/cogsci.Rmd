---
title: "The latent factor structure of developmental change in early childhood"
bibliography: cogsci_ref.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{Anonymous Cogsci Submission}

abstract: >
    Piaget proposed that development proceeded in stages; more recently researchers have proposed modular theories in which different abilities develop on their own timetable. Despite the abundance of theory, there is little empirical work on the structure of developmental changes in early childhood. We investigate this question using a large dataset of parent-reported developmental milestones. We compare a variety of factor-analytic item response theory models and find that variation in development from birth to 55 months of age is best described by a model with three distinct dimensions. We also find evidence that dimensionality increases across age, with the youngest children described by a two-factor model. These results provide a model-based method for linking holistic descriptions of early development to basic theoretical questions about the nature of change in childhood.
    
keywords: >
    child development; milestones; item response theory; model comparison
    
output: cogsci2016::cogsci_paper
header-includes:
- \usepackage{amsmath}
- \usepackage{bm}
- \usepackage{xcolor}
- \newcommand\myworries[1]{\textcolor{red}{#1}}
final-submission: \cogscifinalcopy
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/', 
	echo=F, warning=F, cache=F, message=F, sanitize = T
)

library(tidyverse)
library(rsample)
library(here)
library(mirt)
library(quantregGrowth)
theme_set(theme_classic())
```

```{r load_data}
survey_raw <- read_rds(here("data-clean/survey_raw.rds"))

responses_long <- read_rds(here("data-clean/responses_long.rds"))
responses_wide <- read_rds(here("data-clean/responses_wide.rds"))

milestones <- read_rds(here("data-clean/milestones.rds"))
areas <- read_rds(here("data-clean/areas.rds"))
```

```{r remove_ages}
# remove ages <= 1

by_age <-
	responses_long %>%
	group_by(id) %>%
	summarize(
		age = age[1],
		response = sum(response)
	) %>%
	filter(age > 1)

responses_wide <- responses_wide %>% filter(id %in% by_age$id)
responses_long <- responses_long %>% filter(age > 1)
```

```{r load_results}
n_children <- nrow(responses_wide)
n_milestones <- ncol(responses_wide) - 1

results <-
	bind_rows(
		read_rds(here("data-models", "models_exploratory_figshare.rds")), 
		read_rds(here("data-models", "models_bifactor_figshare.rds"))
	) %>%
	rename(out_of_sample = ll_person_item) %>%
	mutate(
		in_sample = exp(log_lik / n_children)^(1/n_milestones),
		oos = splits_with_log_lik %>% map_dbl(~ sum(.$log_lik_test))
	) %>% 
	select(
		factors, itemtype, in_log_lik = log_lik, in_p = in_sample,
		out_log_lik = oos, out_p = out_of_sample, model_full, 
		fscores, splits_with_log_lik
	)
```

# Introduction

How do young children grow and change? Is child development a single unified process or a host of different processes, each with their own constraints and timescale? Piaget famously proposed a stage theory in which many seemingly distinct mental processes developed in concert through the operation of the same principles across domains [@flavell1963developmental]. In contrast, modern theories propose that different facets of children's mental life develop on their own timetable [@gelman1983preschoolers]. And the grandmother of one author of this paper was known to assert that developmental milestones were in compensatory relationships with one another ("children either walk early or else they talk early"). 

This question is important not only from a theoretical perspective but also for application. The process of assessing children's developmental status critically depends on our assumptions about the nature of that status — in particular, whether there is a single unified process that can be measured via some score derived from subprocesses. In this sense, questions about the nature and structure of development are psychometric questions [@borsboom2005measuring]. Such psychometric analysis investigating the dimensionality of change has been studied extensively in the case of cognitive aging [e.g., @balinsky1941analysis; @li1999test] but has received less attention in early childhood.

Our goal is to describe the psychometric structure of development. We take as our starting point the idea that psychometric models can instantiate hypotheses about psychological structure in ways that can be assessed via their fit to data. We adopt the framework of item response theory (IRT). IRT models allow us to capture how responses to such questions track both with individual children's abilities as well as with the measurement properties of the questions (and underlying milestones). In particular, our interest is in comparing within a family of multidimensional IRT models in order to gain insight into the underlying dimensionality of early childhood development. 

In a standard factor-analytic approach (which multi-dimensional IRT extends), a solution with N factors partitions observed variance into factors, suggesting dimensions of variation in the sample. One substantial complication to this perspective for analyzing developmental data is the issue that the dimensionality of children's variation could itself change developmentally. Indeed, the dedifferentiation hypothesis of cognitive aging — that distinct factors collapse — is such a hypothesis [@de2007revisiting]. To address this challenge, we use a new set of cross-validation methods to investigate changes in dimensionality. 

We use milestone data for our investigation. Global assessment of developmental status via a series of binary questions (e.g., "Can your child walk at least ten steps unassisted?") is both a standard feature of pediatrician visits [@sheldrick2019establishing] and a gold standard for child development in the research and intervention communities [@bayley2009bayley; @bricker1999ages; @mccoy2019preschool]. In such assessments, which are typically but not always conducted via parent report, developmental progress is pooled across domains like motor development or language. Thus, these instruments implicitly assume a unifactorial model, although some also provide subscale scores [@bayley2009bayley].

Unfortunately, these instruments are commercial products, and hence normative data at the item level are typically not available for analysis. In the current paper, we thus analyze a new set of data from a set of 414 milestone questions administered online to a group of 1946 middle-class Mexican parents of children from 0 to 55 months of age. This very comprehensive milestone set allows us to ask questions about how variation in developmental growth can be partitioned across age and face-valid domains (language, cognition, motor, and socio-emotional development).

We first describe our dataset. We then introduce the family of item response models that we use and the way in which we compare performance across these models. These models allow us to consider the overall dimensionality of our dataset, which we then follow up on by looking for evidence of change in dimensionality across development. We end by considering the limitations, implications, and next steps for this work.

# Data

A child’s development can be thought of as the set of developmental milestones that they have reached at a particular point in time. This conceptualization results in data with the same structure as the item response data common to educational measurement. In education, item response data is most typically students responding to test items (i.e., questions) and, in the dichotomous case, getting each question either correct or incorrect. In the context of child development, the child is the “student,” and each developmental milestone is the “item.”

Data were provided by Kinedu, Inc., a developer of parenting applications. We consider the 1946 children between 2 and 55 months of age whose parents responded to all 414 of the developmental milestones. Kinedu, Inc. mapped each milestone to a face-valid group: physical, cognitive, linguistic, or social & emotional. Table \ref{tab:example_milestones} shows the number of developmental milestones in each group along with an example milestone from each group translated to English.

```{r example_milestones}
areas %>%
	filter(paste %in% names(responses_wide)) %>%
	group_by(area) %>%
	mutate(count = n()) %>%
	slice(1) %>%
	ungroup() %>%
	left_join(milestones, by = c("paste" = "code")) %>%
	select(Group = area, `Milestones` = count, spanish = short_name) %>%
	select(-spanish) %>% 
	arrange(desc(`Milestones` )) %>%
	mutate(
		`Example milestone` = 
			c(
				"Stands on their toes",
				"Finds objects on the floor",
				"Babbles to imitate conversations",
				"Complains when play is stopped"
			)
		) %>% 
    knitr::kable("latex", caption = "Developmental milestone groups and examples") %>% 
    kableExtra::kable_styling(font_size = 8, latex_options = "hold_position")
```

Figure \ref{fig:growth} shows the age (in months) and number of completed milestones for each child. At 12 months old, most children have reached about 200 developmental milestones. At 24 months old, most children have reached about 300 developmental milestones. At 48 months old, most children have reached about 375 of the 414 developmental milestones.

```{r growth, fig.cap = 'Number of milestones completed by age with percentile curves. Dots represent individual children.', warning = FALSE}
source(here("R", "predictQR_fixed.R"))

taus <-  c(0.1, 0.25, 0.5, 0.75, 0.9)

model_perc_curves <- 
	gcrq(response ~ ps(age, monotone = 1, lambda = 1000), taus, by_age)

the_ages <- 1:55

newdata <- data.frame(age = the_ages)

preds <- 
	predictQR_fixed(model_perc_curves, newdata = newdata) %>%
	data.frame() %>%
	mutate(age = the_ages) %>%
	gather(Percentile, pred, starts_with("X")) %>%
	mutate(
		Percentile = 
			as.character(as.numeric(str_replace(Percentile, "X", "")) * 100)
	)

by_age %>% 
	ggplot(aes(x = age, y = response)) + 
	geom_jitter(height = 0, width = 0, alpha = .1) +  # was width = 0.2 and alpha 0.3
	geom_line(data = preds, aes(x = age, y = pred, col = Percentile, group = Percentile)) + 
	ylim(0,nrow(milestones)) + 
	xlim(0,60) + 
	ylab("Milestones completed") + 
	xlab("Age (in months)") + 
	ggthemes::scale_color_solarized() + 
	theme(
		legend.position = "bottom", 
        legend.title = element_text(size = 7),
        legend.text = element_text(size= 7),
        legend.key.size = unit(0.5, "cm")
	)
```

# Methods

We frame the assessment of the dimensionality of child development as a model comparison question. 

## Models

Item response theory offers a suite of models with which to model item response data. We adopt the notation used in @chalmers2012mirt. Let $i = 1, \ldots, I$ represent the distinct children and $j = 1, \ldots, J$ the developmental milestones. The item response data is stored in a matrix, $y$, where element $y_{ij}$ denotes if the $i$th child has or has not achieved the $j$th developmental milestone as reported by their parent/guardian. Each model represents the $i$th child's development using $m$ latent factors $\boldsymbol{\theta}_{i}=(\theta_1, \dots, \theta_m)$. The $j$th milestone's discriminations (i.e. slopes) $\boldsymbol{a_j}=(a_1, \dots, a_m)$ capture the latent factor loadings onto that milestone. 

We fit five two-parameter logistic (2PL) models where a child’s development is represented by $m = 1, \ m = 2, \ m = 3, \ m = 4$ and $m = 5$ latent factors. Hereafter, we, for example, refer to a 2PL model with $m = 4$ latent factors as a 4F 2PL model. According to the 2PL model, the probability of a child having achieved a developmental milestone is
$$
P(y_{ij} = 1 | \boldsymbol{\theta_i}, \boldsymbol{a_j}, b_j) = \sigma(\boldsymbol{a}_{j}^{\top}\boldsymbol{\theta_i} + b_j)
$$
where $b_j$ is the milestone easiness (i.e. intercept) and $\sigma(x) = \frac{e^x}{e^x + 1}$ is the standard logistic function. As an example, Figure \ref{fig:icc} shows item characteristic curves from the 1F 2PL model for the items in Table \ref{tab:example_milestones}. Item characteristic curves show how $P(y_{ij} = 1)$ changes as $\theta_i$ changes for a particular item. These curves reveal that babbling is unrelated to development. On the other hand, finding objects on the floor is highly related to development with most children with $\theta_i$ greater than -1.5 having reached this milestone. 

We explored 3PL models, which add a guessing parameter to the 2PL model, but they did not fit better and so we omit them for space, preferring instead to look at the more interesting factor structure. We do include a 1F Rasch model where all of the discriminations, $\boldsymbol{a}_{j}$, are set to 1 for each item.

Each of these models learn the latent factor structure entirely from the data, making them exploratory. The bifactor model offers an alternative specification where each milestone loads onto a general factor $\theta_0$ and a specific factor $\theta_s$ [@cai2011generalized]. The assignment of each developmental milestone to its specific factor is an opportunity to specify the latent factor structure, making the model confirmatory as opposed to exploratory. We map each milestone to its specific factor according to the four developmental milestone groups shown in Table \ref{tab:example_milestones}. For the bifactor model, the probability of a child having achieved a developmental milestone is 
$$
P(y_{ij} = 1 | \theta_0, \theta_s, a_0, a_s) = \sigma(a_0\theta_0 + a_s\theta_s + b_j).
$$

Computing is done in R [@rcore], model fitting in the R package mirt [@chalmers2012mirt], and data wrangling/visualization in the set of R packages known as the tidyverse [@tidy].

```{r icc, fig.cap = "Example item characteristic curves. Babbling is unrelated to a child's development whereas finding objects on the floor is highly related to development."}
# choose items
a <- which("abs_183" == names(responses_wide)) - 1 # -1 because id column
b <- which("babbling_23" == names(responses_wide)) - 1
c <- which("balance_708" == names(responses_wide)) - 1
d <- which("dindep_78" == names(responses_wide)) - 1

# extract probabilities
traceline <- NULL
num <- 1
for(i in c(a, b, c, d)){
	extr.2 <- 
		extract.item(
			results$model_full[[which(results$factors == "1" & results$itemtype == "2PL")]], 
			i
		)
	Theta <- matrix(seq(-4,4, by = .1))
	traceline[[num]] <- probtrace(extr.2, Theta)
	num = num + 1
}

# rename list
names(traceline) <- 
  c(
    "Finds objects on the floor", 
    "Babbles to imitate conversations", 
    "Stands on their toes", 
    "Complains when play is stopped"
  )

# wrangle probabilities
traceline.df <- do.call(rbind, traceline)
item <- rep(names(traceline), each = length(Theta))
l.format <- cbind.data.frame(Theta, item, traceline.df)
l.format$item <- as.factor(l.format$item)
aux <- 
	l.format %>%
	group_by(item) %>%
	slice(which.min(abs(P.1-0.5)))

aux <- aux[order(aux$Theta),]
ord <- as.integer(aux$item)
l.format$item = factor(l.format$item, levels(l.format$item)[ord])

# plot iccs 
l.format %>% 
  as_tibble() %>% 
  ggplot(aes(Theta, P.1)) + 
  geom_line() + 
  facet_wrap(~ item) +
  xlab(expression(theta)) + 
  ylab("Probability of milestone") + 
  theme_classic(base_size = 8) +
  theme(strip.text.x = element_text(size = 5.5))
```

## Model comparison {#modelcompare}

Model comparison in IRT typically uses information criterion such as AIC and BIC [@maydeu2013goodness]. However, these methods are not guaranteed to work with modest sample sizes or misspecification [@mcdonald1995goodness]. Instead, we use a marginalized version of cross-validation. In essence, we partition the data into folds based on the children (i.e. the rows of the item response matrix). Then for each fold, we estimate the item parameters using all but that fold, and calculate the likelihood of that fold by integrating over $g(\theta)$. 

Mathematically and following notation similar to @vehtari2017practical, we partition the data into 6 subsets $y^{(k)}$ for $k = 1, \dots, 6$. Each model is fit separately to each training set $y^{(-k)}$ yielding item parameter estimates which we compactly denote $\Psi_j^{(-k)}$. The predictive (i.e. out-of-sample or cross-validated) likelihood of $y^{(k)}$ is

$$
p(y^{(k)} | y^{(-k)}) = \prod_{i \in i^{(k)}}^{I} \int_\theta \prod_{j=1}^{J} \hat{\text{Pr}}(y_{ij}^{(k)} | \Psi_j^{(-k)}, \theta) g(\theta)d\theta.
$$

The ultimate quantity of interest for each model is the log predictive likelihood for the entire item response matrix, which is defined as

$$
\text{lpl } y = \sum_{k = 1}^{K} \log p(y^{(k)} | y^{(-k)}).
$$

# Results

Table \ref{tab:results} shows the number of parameters, the in-sample log likelihood (which neccessarily increases with more parameters), and the $\text{lpl } y$ defined in the [model comparison section](#modelcompare). The 3F 2PL model performs best which is evidence that child development between the ages of 2 and 55 months follows a multidimensional path. 

```{r results}
# for latek in kables
# https://stackoverflow.com/questions/49416492/latex-formulas-or-symbols-in-table-cells-using-knitr-and-kableextra-in-r-markdow

results %>%
	mutate(npars = model_full %>% map_int(~ .@Model$nestpars)) %>% 
	arrange(npars) %>%
	mutate(
		model = c("1F Rasch", "1F 2PL", "2F 2PL", "Bifactor", "*3F 2PL*", "4F 2PL", "5F 2PL")
	) %>%
	select(
		model, parameters = npars, `log-likelihood (in-sample)` = in_log_lik, 
		`lpl y (out-of-sample)` = out_log_lik
	) %>% 
	knitr::kable("latex", caption = "Model performance: The 3F 2PL performs best as measured by lpl y") %>% 
    kableExtra::kable_styling(font_size = 8, latex_options = "hold_position")
```

## Understanding the latent factor structure

To understand each of the three factors in the best performing model, we fit the model to the the full dataset. We then estimate the factor loadings (i.e. discriminations or slopes) using a varimax rotation. The varimax rotation results in orthogonal and, therefore, more interpretable factors [@kaiser1959computer]. Figure \ref{fig:factorloadings} shows the distribution of factor loadings for each group on each of the three factors. The first factor loads mainly on cognitive and linguistic milestones. The second factor is a combination of each of the groups with the strongest loadings on the physical and social & emotional milestones. The third mainly load positively on linguistic milestones and, interestingly, negatively on physical milestones. 

```{r getfactorloadings, include = FALSE}
mod_3F_2PL_summary <- 
	summary(
		results$model_full[[which(results$factors == "3")]], 
		rotate = "varimax"
	)

mod3_3F_2PL_loadings <-
    mod_3F_2PL_summary$rotF %>%
    as_tibble() %>%
    mutate(paste = row.names(mod_3F_2PL_summary$rotF)) %>%
    left_join(areas)
```

```{r factorloadings, fig.cap = 'Factor loadings by group'}
mod3_3F_2PL_loadings %>%
    select(Group = area, F1, F2, F3) %>%
    rename(`Factor 1` = F1, `Factor 2` = F2, `Factor 3` = F3) %>% 
    gather(var, val, -Group) %>%
    mutate(val = -val) %>%
    ggplot(aes(x = val, fill = Group)) +
    ggridges::geom_density_line(alpha = 0.5) +
    facet_wrap(~ var, ncol = 1) +
    labs(
        x = "Factor loading (i.e. discrimination or slope)",
        y = "Density"
    ) +
    theme_classic(base_size = 8) + 
    theme(legend.position = "bottom", 
          legend.title = element_blank(),
          legend.text = element_text(size = 6),
          legend.key.size = unit(0.4, "cm")) +
    scale_fill_viridis_d()
```

```{r getfactorscores}
# that last mutate requires ordering to be the same, which it is:
# all(responses_wide$id == by_age$id)

mod_3F_2PL_fscores <- 
	fscores(
		results$model_full[[which(results$factors == "3")]], 
		rotate = "varimax"
	) %>% 
	as_tibble() %>% 
	mutate(age = by_age$age)
```

We also estimate the factor scores for each child using expected a posteriori (EAP) with a three dimensional standard normal distribution [@embretson2013item]. Figure \ref{fig:factorscores} shows the relationship between age and factor score for each factor. The first factor, perhaps unsurprisingly, has a high correlation (r = `r abs(round(cor(mod_3F_2PL_fscores$F1, mod_3F_2PL_fscores$age), 2))`) with age. The second factor has a strong association with age from 2 to 16 months but thereafter is unrelated to age. By and large, the third factor does not have any association with age.

```{r factorscores, fig.cap = 'The first factor is highly associated with age'}
mod_3F_2PL_fscores %>%
    rename(`Factor 1` = F1, `Factor 2` = F2, `Factor 3` = F3) %>% 
    gather(var, val, -age) %>%
    mutate(val = -val) %>%
    ggplot(aes(x = age, y = val)) +
    geom_point(alpha = 0.1) +
    facet_wrap(~ var, ncol = 1) +
    geom_smooth() +
    labs(
        x = "Age (in months)",
        y = "Factor score"
    )
```

## Dimensionality across the age-span

For the entire dataset, we’ve shown evidence that the 2PL model with 3-factors performs best. But is this latent factor dimensionality consistent across age-span? For example, perhaps for very young children 1-factor is sufficient and then later on 2- and then 3-factors become valuable. We take two approaches to assessing the dimensionality of child development across the age-span. First, we examine the performance of each of the models by age. Second, we partition the data by age and use the same cross-validation procedure to find the best fitting model in each partition.

## Full model {#full}

Figure \ref{fig:byage} displays the mean cross-validated log likelihood for each model by age, which comes from the k-fold cross-validation described in the [model comparison](#modelcompare) section. For each student, we calculate the marginalized out-of-sample likelihood based on the item parameters $\Psi_j^{(-k)}$ from fitting the model to $y^{(-k)}$, the folds of data that do not include the student. As a reminder, students are assigned to folds randomly and not by age. 

Figure \ref{fig:byage} shows how both the 3F 2PL and bifactor models compare to the 2F 2PL model in terms of cross-validated log likelihood for each age. The 2F 2PL outperforms both models for children younger than 7 months old. For children older than 11 months old, both the 3F 2PL and bifactor models outperform the 2F 2PL model with the 3F 2PL model tending to perform best. 

```{r getbyage, include = FALSE}
source(here("R", "splits_by_age.R"))
source(here("R", "calc_log_lik_ghq2_student.R"))

model_by_age <-
    results %>%
	mutate(model = c("1F Rasch", "1F 2PL", "2F 2PL", "3F 2PL", "4F 2PL", "5F 2PL", "Bifactor")) %>% 
    select(model, factors, itemtype, splits_with_log_lik) %>%
    mutate(oos = splits_with_log_lik %>% map(splits_with_ll_to_oos))
```

```{r byage, fig.cap = 'Comparing the 4F 2PL and Bifactor models to the 2F 2PL'}
model_by_age %>%
	filter(model %in% c("2F 2PL", "3F 2PL", "Bifactor")) %>%
	mutate(model = factor(model, levels = c("2F 2PL", "Bifactor", "3F 2PL"))) %>% 
	select(model, oos) %>%
	unnest(oos) %>%
	mutate(oos = log(oos)) %>%
	group_by(model, age) %>%
	summarize(m = mean(oos)) %>%
	ungroup() %>%
	spread(model, m) %>% 
	mutate(`3F 2PL` = `3F 2PL` - `2F 2PL`, Bifactor = Bifactor - `2F 2PL`) %>% 
	select(-`2F 2PL`) %>% 
	gather(Model, m, -age) %>% 
	ggplot(aes(x = age, y = m, color = Model)) +
	geom_point(size = 0.75) +
	geom_path(size = 0.35) +
	geom_hline(yintercept = 0, linetype = "dashed") +
	labs(x = "Age (in months)", y = "Mean difference in log likelihood") +
	theme_classic(base_size = 10) + 
	theme(
		legend.position = "bottom", 
		legend.title = element_blank(),
		legend.text = element_text(size = 8),
		legend.key.size = unit(0.5, "cm")
	) +
    ggthemes::scale_color_solarized()
```

## Age-partitioned models

As another method of examining the dimensionality of child development across the age span, we create four partitions of the data based on the ages of the children. We then cross-validate the 2PL models independently in each partition. This allows us to examine the dimensionality for each age group separately. For each age partition, we drop milestones where less than 2.5% or greater than 97.5% of children have reached the milestone. We drop these milestones because they contain little information and make the models less stable. This process results in, for example, 432 children and 359 milestones in the 13-24 month old partition.

Figure \ref{fig:partage} shows the results of this analysis. Consistent with our findings in the [previous section](#full), the best fitting model contains a lower dimensional factor structure for younger children. The best fitting model is the 2F 2PL for the partition of data containing children two to 12 months old, whereas the best fitting model is the 3F 2PL for the partitions containing older children.

```{r partage, fig.cap = '2F 2PL best for young kids; 3F 2PL best for older kids'}
age_partitioned_models <- read_rds(here("data-models", "age_partitioned_models_figshare.rds"))

add_lltest <- function(df){
  df %>% 
  mutate(
    log_lik_test = 
      splits_with_log_lik %>% 
      map_dbl(~ sum(.$log_lik_test))
  )
}

age_partitioned_models %>% 
	transmute(
		years = c("2-12 months", "13-24 months", "25-36 months", "37+ months"),
		kids = age_partitioned_models$d_mat_filter %>% map_int(nrow),
		items = age_partitioned_models$d_mat_filter %>% map_int(ncol),
		models = models %>% map(add_lltest)
	) %>% 
	unnest(models) %>% 
	select(years, kids, items, factors, itemtype, log_lik, log_lik_test) %>% 
	mutate(var = as_factor(paste0(years, " ", kids, " kids", " ", items, " milestones"))) %>% 
	filter(itemtype == "2PL") %>% 
	ggplot(aes(x = factors, y = log_lik_test)) +
	geom_point() +
	geom_line(aes(group = years)) +
	facet_wrap(~ var, ncol = 1, scales = "free") +
	labs(
		x = "m-factor 2PL model",
		y = "Cross-validated log likelihood"
	) +
  theme_classic(base_size = 8)
```

# Discussion

Is child development a single unified process or a host of different processes? Stage theories assume synchronization in developmental changes across distinct domains like language, social/emotional development, and cognition. In contrast, more modern modular theories tend to assume that particular aspects of development proceed "on their own schedule" [@spelke1992origins]. Here, inspired by psychometric studies of age-related changes in cognition, we explored this issue in a large dataset of children's developmental milestones. Our premise was that understanding the nature of variation in milestones could help shed light on whether children's developmental change covaries across domains within a single factor or whether it is split into multiple factors.

Using multi-factor item response theory models and a new cross-validation method for model comparison, we found that a three-factor model best described developmental variation across the first 55 months. While the first factor described a large amount of shared variation in development, the structure of these factors did suggest some differentiation between cognitive/linguistic development, physical development, and socio/emotional development. Further, we found that the dimensionality of variation increased developmentally: 2--12 month-olds were best described by a two-dimensional model, while older groups were best described by a three-dimensional model. This analysis provides tentative support for a developmental differentiation hypothesis, where different domains of development vary across individuals in a way that is increasingly more independent over age.

Our study has a number of limitations that should inform future work. Our dataset is cross-sectional, meaning that we are only describing variation across individuals rather than the coherence of factors within individuals. Second, we relied on parent report, which can have significant biases and limitations, especially in its precision regarding capacities that are difficult to observe [e.g., cognitive abilities; @feldman2000measurement; @wordbank]. Third, our data come from a very specific population group (middle- and upper-class Mexican parents whose children were in group care) and hence caution is warranted in generalizing to other populations.

This work has significant practical implications: Measures of developmental change should not assume that a single score captures all of the variance in developmental change. Thus, understanding the generality of our conclusions is an important practical goal that could affect the structure of a variety of standardized developmental inventories in broad clinical and research use.

The nature of developmental variation is of core importance to our theorizing about the mechanisms of child development. Yet this variation has often been assumed to be unifactorial or multifactorial without formal evaluation. Our work here takes a first step towards using psychometric models to evaluate this dimensionality empirically.

# Acknowledgements

```{r}
# Thanks to Kinedu, Inc. for support and data sharing and to George Kachergis, Alex Carstensen, and Ann Weber for comments on early versions of this paper.
```

[BLINDED]

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
